<imageModule category="Image" creation="2015-06-04 11:35:34.195 UTC" deleted="false" description="Hadoop master node" isBase="false" isLatestVersion="true" lastModified="2015-07-01 14:35:00.878 UTC" loginUser="root" logoLink="https://slipstream.sixsq.com/images/modules-logos/hadoop.svg" moduleReferenceUri="module/examples/images/centos-6" parentUri="module/apps/Hadoop" platform="centos" shortName="master" version="1715">
   <authz groupCreateChildren="false" groupDelete="false" groupGet="true" groupPost="false" groupPut="true" inheritedGroupMembers="true" owner="sixsq" ownerCreateChildren="true" ownerDelete="true" ownerGet="true" ownerPost="true" ownerPut="true" publicCreateChildren="false" publicDelete="false" publicGet="true" publicPost="false" publicPut="false">
      <groupMembers>
         </groupMembers>
   </authz>
   <commit author="sixsq">
      <comment>Initial version of this module</comment>
   </commit>
   <cloudNames length="12">
      </cloudNames>
   <runs count="0" limit="20" offset="0" totalCount="0" />
   <targets>
      <target name="report" />
      <target name="onvmremove" />
      <target name="execute">#!/bin/bash -x


#disactive le firewall
service iptables stop


yum -y install wget

#to fix this error : 
#The following hosts have Transparent Huge Pages (THP) enabled. THP should be disabled to avoid potential Hadoop performance issues.
echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled 
echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag


#clean all
yum -y clean all


#get dns_domain from ambari server
dns_domain=`ss-get --timeout 480 hadoop_installer.1:dns_domain`


#Set &amp; Send Hostname (DNS) to hadoop ambari server
master_id=`ss-get --timeout 480 id`
hostname="master${master_id}.${dns_domain}"
ss-set node_hostname "$hostname"


#set hostname
hostname $hostname

#Get ip master node
ip_master=`ss-get --timeout 480 hostname`


truncate -s 0 /etc/hosts
cat &lt;&lt; EOF &gt;&gt; /etc/hosts
$ip_master  $hostname
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
EOF

 # var=$(hostname) 
truncate -s 0 /etc/sysconfig/network
cat &lt;&lt; EOF &gt;&gt; /etc/sysconfig/network
NETWORK=yes
NETWORK_IPV6=no
HOSTNAME=$hostname
EOF



ip_hadoop_installer=`ss-get --timeout 480 hadoop_installer.1:hostname`

sed -i "1i nameserver $ip_hadoop_installer" /etc/resolv.conf

#echo "nameserver $ip_ambari_server" &gt; /etc/resolv.conf
#cat &lt;&lt; EOF &gt;&gt; /etc/resolv.conf
#nameserver $ip_ambari_server
#EOF



chkconfig iptables off
chkconfig ip6tables off



truncate -s 0 /etc/sysconfig/selinux
cat &lt;&lt; EOF &gt;&gt; /etc/sysconfig/selinux
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#       enforcing - SELinux security policy is enforced.
#       permissive - SELinux prints warnings instead of enforcing.
#       disabled - SELinux is fully disabled.
SELINUX=disabled
# SELINUXTYPE= type of policy in use. Possible values are:
#       targeted - Only targeted network daemons are protected.
#       strict - Full SELinux protection.
SELINUXTYPE=targeted
EOF




# Generate keys for master
ssh-keygen -f /root/.ssh/id_rsa -N ""


# configuration for ssh access between hadoop_master and slaves
ss-set master_id_rsa64 `cat chmod 604 /root/.ssh/id_rsa.pub | base64 --wrap 0`




#Configuration for ssh access between ambari_server and hadoop_nodes(master&amp;slaves)
hadoop_installer_public_key=`ss-get --timeout 480 hadoop_installer.1:hadoop_installer_id_rsa64 | base64 -d`
cat &lt;&lt; EOF &gt;&gt; /root/.ssh/authorized_keys
$hadoop_installer_public_key
EOF
#hadoop_installer Keyscan
hadoop_installer_ip_hostname=`ss-get --timeout 480 hadoop_installer.1:hostname`
hadoop_installer_node_hostname=`ss-get --timeout 480 hadoop_installer.1:hadoop_installer_hostname`
ssh-keyscan $hadoop_installer_ip_hostname &gt;&gt; /root/.ssh/known_hosts
ssh-keyscan $hadoop_installer_node_hostname  &gt;&gt; /root/.ssh/known_hosts





#Configuration for ssh access between hadoop_master et hadoop_slaves
for (( i=1; i &lt;= `ss-get slave:multiplicity`; i++ )); do

hadoop_salve_public_key=`ss-get --timeout 480 slave.$i:slave_id_rsa64 | base64 -d`
cat &lt;&lt; EOF &gt;&gt; /root/.ssh/authorized_keys
$hadoop_salve_public_key
EOF

#Slaves Keyscan 
ip_hostname=`ss-get --timeout 480 slave.$i:hostname`
node_hostname=`ss-get --timeout 480 slave.$i:node_hostname`
ssh-keyscan $ip_hostname &gt;&gt; /root/.ssh/known_hosts
ssh-keyscan $node_hostname  &gt;&gt; /root/.ssh/known_hosts


done





#**********************************************   Blue print section   *************************************************

#Download the Ambari repository on the Ambari Server host
cd /etc/yum.repos.d/
wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo


#Install Ambari Agent from the public Ambari repository
yum -y install ambari-agent

#recieve Ambari server_hostname (DNS) 
hadoop_installer_hostname=`ss-get --timeout 480 hadoop_installer.1:hadoop_installer_hostname`


#Set the Ambari Server on the Ambari Agent.
truncate -s 0 /etc/ambari-agent/conf/ambari-agent.ini
cat &lt;&lt; EOF &gt;&gt; /etc/ambari-agent/conf/ambari-agent.ini
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific

[server]
hostname=$hadoop_installer_hostname
url_port=8440
secured_url_port=8441

[agent]
prefix=/var/lib/ambari-agent/data
tmp_dir=/var/lib/ambari-agent/data/tmp
;loglevel=(DEBUG/INFO)
loglevel=INFO
data_cleanup_interval=86400
data_cleanup_max_age=2592000
data_cleanup_max_size_MB = 100
ping_port=8670
cache_dir=/var/lib/ambari-agent/cache
tolerate_download_failures=true
run_as_user=root

[command]
maxretries=2
sleepBetweenRetries=1

[security]
keysdir=/var/lib/ambari-agent/keys
server_crt=ca.crt
passphrase_env_var_name=AMBARI_PASSPHRASE

[services]
pidLookupPath=/var/run/

[heartbeat]
state_interval=6
dirs=/etc/hadoop,/etc/hadoop/conf,/etc/hbase,/etc/hcatalog,/etc/hive,/etc/oozie,
  /etc/sqoop,/etc/ganglia,
  /var/run/hadoop,/var/run/zookeeper,/var/run/hbase,/var/run/templeton,/var/run/oozie,
  /var/log/hadoop,/var/log/zookeeper,/var/log/hbase,/var/run/templeton,/var/log/hive
; 0 - unlimited
log_lines_count=300
EOF




#Start the Agent to initiate registration to server
ambari-agent start

#*************************************************************************************************************************



#**********************************************   vnc server install  ****************************************************

#ss-set statecustom 'Centos update packages...'
## update all of the packages to the latest releases
#yum update


ss-set statecustom 'Vnc server install...'
#install gnome desktop environment
yum -y groupinstall Desktop


#install VNC package along with dependencies
yum -y install tigervnc-server pixman pixman-devel libXfont


# create a random password for vnc_user
vnc_password=`openssl rand -base64 6`
crypt_vnc_password=`echo $vnc_password | openssl passwd -crypt -stdin`


# publish password so vnc_user can log in
# will be visible in machine parameters in SlipStream interface
ss-set vnc_password $vnc_password

#Login for vnc user
ip_node=`ss-get --timeout 480 hostname`
ss-set vnc_address "${ip_node}:1"

#Create VNC password for the root
mkdir -p /root/.vnc
echo $vnc_password | vncpasswd -f &gt; /root/.vnc/passwd
chmod 0400 /root/.vnc/passwd


#configure resolution for the root 
cat &gt; /etc/sysconfig/vncservers &lt;&lt;'EOF'
VNCSERVERS="1:root"
VNCSERVERARGS[1]="-geometry 1000x800"
EOF


ss-set statecustom 'Vnc server start...'
#Start vnc service
service vncserver start


#ouvre le port utilis&#233; par vnc
iptables -I INPUT 4 -j ACCEPT -p tcp --dport 5900:5902 


# set the customstate to inform user that everything's ready
ss-set statecustom 'VNC server Ready!'






#*************************************************Install SlipStream-client ***********************************************************


#install pip
yum install -y python-pip


#install slipstream-client
pip install slipstream-client


#install httplib2
pip install httplib2


#cmd to add a new slave
mkdir /root/hadoop-examples
cd /root/hadoop-examples
touch add-slave
touch remove-slave


#get run_id
run_id=`cat /tmp/slipstream.context | grep diid | grep -Po '[^ ]+$'`

cat &lt;&lt; EOF &gt;&gt; /root/hadoop-examples/add-slave
#!/bin/sh
read -p "Enter slave number to add : " slave_nb
ss-node-add $run_id slave $slave_nb
EOF


cat &lt;&lt; EOF &gt;&gt; /root/hadoop-examples/remove-slave
#!/bin/sh
read -p "Enter slave_id to remove: " slave_id
ss-node-remove $run_id slave $slave_id
EOF

chmod +x /root/hadoop-examples/add-slave
chmod +x /root/hadoop-examples/remove-slave


#**********************************************  Fix Bug on hadoop installation *****************************************************


install_progress=`ss-get --timeout 480 hadoop_installer.1:Hadoop_install_progress`

ss-set statecustom 'Hadoop install ...'
while [ "$install_progress" != '100.' ]
do
sleep 10   
install_progress=`ss-get --timeout 480 hadoop_installer.1:Hadoop_install_progress`
ss-set statecustom "In progress: ${install_progress} %"
done


#After all installation &amp; configuration we will Fix bug on hadoop

#setup user hdfs directory
#sudo -u hdfs  hdfs  dfs -mkdir /user/$USER
#sudo -u hdfs  hdfs  dfs -chown $USER  /user/$USER

#Juste modification &#163;USER par root pour qu'il prendre le valeur root 
sudo -u hdfs  hdfs  dfs -mkdir /user/root
sudo -u hdfs  hdfs  dfs -chown root  /user/root


#Problem:  During mapreduce we get this Error : File does not exist: hdfs://...../hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz
#cause: missing file

#Fix: (adjust the version strings)
sudo -u hdfs hdfs dfs -mkdir -p /hdp/apps/2.2.4.2-2/mapreduce
sudo -u hdfs  hdfs dfs -put  /usr/hdp/current/hadoop-client/mapreduce.tar.gz   /hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz

#************************************************************************************************************************************


#********************** ADD an example application to validate Hadoop deployment works (Calcul of pi number) ************************

mkdir /root/hadoop-examples
cd /root/hadoop-examples
touch pi-calcul

cat &lt;&lt; EOF &gt;&gt; /root/hadoop-examples/pi-calcul
#!/bin/sh
echo "-------------------------------------------------------------------------------"
echo "Im an example application to demonstrate that the Hadoop infrastructure works"
echo "I will estimate the Pi number ..."
echo "-------------------------------------------------------------------------------"
echo ""
yarn jar /usr/hdp/2.2.6.0-2800/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0.2.2.6.0-2800.jar pi 5 500
EOF

chmod +x /root/hadoop-examples/pi-calcul

#***************************************************************************************************************************************

# set the deployment SSH url
hostname=`ss-get hostname`
url_ssh="ssh://root@${hostname}"
ss-set url.ssh "${url_ssh}"


# set the customstate to inform user that everything's ready
ss-set statecustom 'Master node Ready!'</target>
      <target name="onvmadd" />
   </targets>
   <packages />
   <prerecipe />
   <recipe />
   <cloudImageIdentifiers />
   <parameters>
      <entry>
         <string>network</string>
         <parameter category="Cloud" description="Network type" isSet="true" mandatory="true" name="network" order="0" order_="0" readonly="false" type="Enum">
            <enumValues length="2">
               <string>Public</string>
               <string>Private</string>
            </enumValues>
            <value>Public</value>
            <defaultValue>Public</defaultValue>
         </parameter>
      </entry>
      <entry>
         <string>master_id_rsa64</string>
         <parameter category="Output" description="base64 encoded ssh public key for hadoop master" isSet="false" mandatory="false" name="master_id_rsa64" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>hostname</string>
         <parameter category="Output" description="hostname/ip of the image" isSet="false" mandatory="true" name="hostname" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>vnc_password</string>
         <parameter category="Output" description="vnc server password" isSet="false" mandatory="false" name="vnc_password" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>extra.disk.volatile</string>
         <parameter category="Cloud" description="Volatile extra disk in GB" isSet="false" mandatory="true" name="extra.disk.volatile" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>instanceid</string>
         <parameter category="Output" description="Cloud instance id" isSet="false" mandatory="true" name="instanceid" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>vnc_address</string>
         <parameter category="Output" description="vnc server address" isSet="false" mandatory="false" name="vnc_address" order="0" order_="0" readonly="false" type="String" />
      </entry>
      <entry>
         <string>node_hostname</string>
         <parameter category="Output" description="Node Hostname (DNS)" isSet="false" mandatory="false" name="node_hostname" order="0" order_="0" readonly="false" type="String" />
      </entry>
      </parameters>
   <notes length="0" />
</imageModule>